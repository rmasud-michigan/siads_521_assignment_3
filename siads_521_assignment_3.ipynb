{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a31ac3a-4593-462e-8290-9196a8db9f34",
   "metadata": {},
   "source": [
    "#### Run First to detect changes and reload modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f3f7e0-db44-466b-b0b8-23f3a0da72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable auto reload\n",
    "%load_ext autoreload\n",
    "# option 2 -  Reloads all modules except those explicitly excluded.\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd5a83-874d-4f15-8ead-e22db3dc06d4",
   "metadata": {
    "panel-layout": {
     "height": 84.3906,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## Assignment 3 \n",
    "\n",
    "Radif Masud "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7a6dc-47ca-4521-80bc-aa3b02b8ff89",
   "metadata": {
    "panel-layout": {
     "height": 50.7969,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## Visualization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579f480f-2cc2-42c3-ac14-f70f37d357e8",
   "metadata": {
    "panel-layout": {
     "height": 10,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "> As part of the visualization layer of the workbook, I will have examples of different plots using libraries that are incrediably powerful and can bring large dataset together quickly. I am hoping as you look throgh this workbook you see the power of these visualizations and get a sense of why a picuture can be worth a thousand words.\n",
    "\n",
    "> The visualizations will use primarily matplotlb and seaborn as primay rendering tools. With this workbook, we will see bar plots for large aggregate data. For example, single value aggregation by year, can be easily presented in vertical bar graph. A bar graph can good for quick aggregate however details are better suited in other plots. Another plot that is in this workbook is a jitter scattergraph. This a very powerful represent on data that has potentially heavy density at a coordinates and provide a point visual on the volume of occurances at each (x,y). One downside to a jitter scattergraph is the zoom level on the data. That means if we are looking at a dataset that has millions of rows with hundreds of thousands of occurances, the jitter may still resemble a continues wide line. Filtering on the data can help on that to get down to a reasonable view projection.\n",
    "> Another visualization that is part of this workbook utilizes kernel density estimation via the seaborn libray. This combined with a normal distribution can provide a useful visualization to highlight kurtosis in the data.\n",
    "> The last visualization is a heatmap to provide categorical analysis. The heatmap \n",
    "> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fcdfdb-ac1d-428f-808b-0a4d7c04ee40",
   "metadata": {
    "panel-layout": {
     "height": 50.7969,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## Visualization Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a25a98-effc-448e-b6f2-aaf3f0baa85f",
   "metadata": {
    "panel-layout": {
     "height": 10,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "\n",
    "> As part of our documentation, we want to call out the required libraries that make this workbook functional, how to install them and what they provide. As part of the workbook design you may not see the being directly referenced and that is by design to keep this workbook clean. Where possible the various libraries being referenced will be encapsulated in functions within the modules contained within this workbook's directory. As part of this workbook, no specific version of these libraries is required. The latest available should work for the functions.\n",
    "\n",
    "\n",
    "| Module     | Install                                                       | Description                                                                                                    |\n",
    "|------------|---------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n",
    "| requests      | <div style=\"text-align: left\">`pip install requests` </div>      | <div style=\"text-align: left\">Python based HTTP client </div>             |\n",
    "| pandas     | <div style=\"text-align: left\">`pip install pandas` </div>     | <div style=\"text-align: left\">Powerful data structures for data analysis, time series, and statistics  </div>  |\n",
    "| matplotlib | <div style=\"text-align: left\">`pip install matplotlib` </div> | <div style=\"text-align: left\">Python plotting package </div>                                                   |\n",
    "| seaborn    | <div style=\"text-align: left\">`pip install seaborn` </div>    | <div style=\"text-align: left\">Statistical data visualization </div>                                            |\n",
    "| numpy      | <div style=\"text-align: left\">`pip install numpy` </div>      | <div style=\"text-align: left\">Foundational module for dealing and working with large arrays </div>             |\n",
    "| scipy      | <div style=\"text-align: left\">`pip install scipy` </div>      | <div style=\"text-align: left\">Fundamental algorithms for scientific computing in Python </div>             |\n",
    "| panel      | <div style=\"text-align: left\">`pip install panel` </div>      | <div style=\"text-align: left\">Powerful data exploration & web app framework for Python </div>             |\n",
    "| ipython      | <div style=\"text-align: left\">`pip install ipython` </div>      | <div style=\"text-align: left\">IPython: Productive Interactive Computing </div>             |\n",
    "| ipywidgets      | <div style=\"text-align: left\">`pip install ipywidgets` </div>      | <div style=\"text-align: left\">Jupyter interactive widgets </div>             |\n",
    "\n",
    "\n",
    "An additional easy way to  install all of these in one command is via the requirements.txt. The provided file contains all the detected dependecies and their versions at the time this workbook was created. \n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8786769d-4f9a-422d-8b12-fc4cc1b5c67f",
   "metadata": {
    "panel-layout": {
     "height": 50.7969,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a04e5da08a5de5",
   "metadata": {},
   "source": [
    "> In this workbook, I wanted to present different graphical representations of traffic crash information for the City of Chicago. Chicago is an incredibly large city in Illinois and one in the top 5 of the largest cities in the United States. Naturally there are a lot of visitors and residents the come to the city. Chicago is also a well established hub for mass transit that supports many of the residents and commutars that come to the city to visit or work. However like many cities in the United States, accidents happen. As part of this workbook, I will use different visualization techniques and processing of the data the abilities the libraries used.  Some of the graphics will be more straightforward and other will grow in the depth of complexity.\n",
    "\n",
    "> Some of the goals of this workbook are to provide a user set of tools that can be used as a point of refence as well. Code specific functionality as well techniques on how to implement them within python and a jupyter environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b6c39-2780-46bc-80a4-e41b81181723",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    " ### Supporting Code - Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c5c25-5504-4fab-84cf-1a04c3b09d98",
   "metadata": {},
   "source": [
    "> To create clean seperation, the code functions mentioned in the cell can be located in:\n",
    "\n",
    "\n",
    "```\n",
    "./\n",
    "├─ modules/\n",
    "│  ├─ reusable.py\n",
    "│  ├─ __init__.py\n",
    "├─ assets/\n",
    "│  ├─ data/\n",
    "│  ├─ chicago_traffic_crashes.csv\n",
    "├─ static/\n",
    "│  ├─ *.png\n",
    "```\n",
    "\n",
    "> * The modules directory contains all the code referenced in this workbook. The resuable.py file contains all the functions referenced in the working book and is brought into the workbook via this line of code:\n",
    "\n",
    "```\n",
    "from modules.reusable import *\n",
    "```\n",
    "> * The assets/data folder contains any referencable datasets and is also where downloaded data should be placed if done manually or via the functions.\n",
    "> * The static directory is used to store static images referenced in this workbook.\n",
    "\n",
    "\n",
    "**If you decide to add functions to reusable.py or modify them, depending on your python and jupyter setup, you may need to restart your kernel to get the latest changes pulled in and reloaded**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9e6302-cf6a-49bd-830f-dd2cf542cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***********************************************************\n",
    "# execute the primary import of the required libaries\n",
    "# this will import all of the library functions needed for:\n",
    "# * data and visualization\n",
    "#***********************************************************\n",
    "\n",
    "# reusable functions\n",
    "from modules.reusable import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99694085-f704-4428-8f6a-4ccf096c45c9",
   "metadata": {},
   "source": [
    "### Data - Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b380b-cb5e-49e6-956b-c5d0413d59ec",
   "metadata": {},
   "source": [
    "> This workbook contains a reusable function that will connect to a free and open souce dataset available from the City of Chicago.\n",
    "Via the requests module, this workbook can call and invoke the download the file like the web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ab973-24a8-493e-9ac4-53684830e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data is not present in the assets/data we will retrieve it\n",
    "# the Dataset can be large from the City of Chicago and depending on you internet connection, could take a minute.\n",
    "download_chicago_crashdata()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f36a6d-6444-4bc3-8a62-01b345e358da",
   "metadata": {},
   "source": [
    "### Data - Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f04cdb-a5e5-4a91-b6c7-8df5a6a27b94",
   "metadata": {},
   "source": [
    "> As part of the data presentation, pandas offers a robust set of tools and combined with programatic abilities in python, we can enhance a dataset to reduc UI and code noise. As part of the dataset from the city of Chicago data coming in is in the form of CSV and times we will want to have some of the fields sub parsed or in a more performant format like a date time. As part of the reuseable.py, those transoformations and mutations will occur in function called etl_crash_data.\n",
    "\n",
    "**example code**\n",
    "```\n",
    "    \"\"\"\n",
    "    Does some transformation and extraction of additional data into our crash dataset that can be helpful for visualizations\n",
    "    Args:\n",
    "\n",
    "        df (pandas dataframe): Dataframe to modify and pass back to the caller\n",
    "    Returns:\n",
    "        A modified dataframe with some additional enriched data.\n",
    "    \"\"\"\n",
    "\n",
    "    # pass in the format - faster to load seconds over minutes.\n",
    "    df['CRASH_DATE'] = pd.to_datetime(df['CRASH_DATE'],format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "    df['CRASH_YEAR'] = df['CRASH_DATE'].dt.year\n",
    "    df['CRASH_YEAR'] = df['CRASH_DATE'].dt.year\n",
    "    df['CRASH_DAY_NAME'] = df['CRASH_DATE'] .dt.day_name()\n",
    "    df['CRASH_MONTH_NAME'] = df['CRASH_DATE'].dt.month_name()\n",
    "\n",
    "    # we want to ensure our numeric data is properly set to 0 if NaN is encountered\n",
    "    # not doing this can impede or cause errors in heatmaps and histograms\n",
    "    df['INJURIES_TOTAL'] = df['INJURIES_TOTAL'].fillna(0)\n",
    "    df['INJURIES_FATAL'] = df['INJURIES_FATAL'].fillna(0)\n",
    "    df['INJURIES_INCAPACITATING'] = df['INJURIES_INCAPACITATING'].fillna(0)\n",
    "    df['INJURIES_NO_INDICATION'] = df['INJURIES_NO_INDICATION'].fillna(0)\n",
    "    df['INJURIES_NON_INCAPACITATING'] = df['INJURIES_NON_INCAPACITATING'].fillna(0)\n",
    "    df['INJURIES_UNKNOWN'] = df['INJURIES_UNKNOWN'].fillna(0)\n",
    "    df['INJURIES_REPORTED_NOT_EVIDENT'] = df['INJURIES_REPORTED_NOT_EVIDENT'].fillna(0)\n",
    "\n",
    "    return df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8f8bf-a2c3-4fa0-8da0-f554e0b0eb01",
   "metadata": {},
   "source": [
    "### Data - Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9da76-2618-4fba-bb2a-81e7517527f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data into our local variable\n",
    "# we will pass that to our reusable plotting functions throughout the workbook\n",
    "# this dataset will have the etl from etl_crash_data applied to it\n",
    "df_chicago_data = get_chicago_crash_data()\n",
    "\n",
    "# let's take a peek at the data\n",
    "df_chicago_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b5135-4361-40fe-8f57-76e37a91a697",
   "metadata": {},
   "source": [
    "### Figure 1 - Plot of Annualized Traffic Crashes All Years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fe83a-2808-4c39-83a9-e3e461293aca",
   "metadata": {},
   "source": [
    "> We want to better understand how the city of Chicago is doing with car crashes year over year. We can accomplish this with by aggregating our dataset on the CRASH_YEAR and determine overall counts and further annotate our plot with the % of change. \n",
    "> * Questions we can look to answer?\n",
    ">   * Are there holes in the data?\n",
    ">   * Are outliers present or are there extreme changes + or - ?\n",
    ">     * Are those changes explainable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75d909-2509-450d-9a36-01c4b5c275c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our primary column of aggregation is the CRASH_YEAR\n",
    "# we can look at aggregate of the instance counts by year\n",
    "cell_crash_counts_by_year = df_chicago_data['CRASH_YEAR'].value_counts().sort_index()\n",
    "\n",
    "# via describe we get our high level stats.\n",
    "cell_crash_counts_by_year.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb7c6b-a483-4870-98e5-3c6c1162a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the reusable function and pass in our dataset\n",
    "plot_crash_count_by_year(df_chicago_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa86e56-abb7-4c0f-84bb-ee1c22201e15",
   "metadata": {},
   "source": [
    "#### Crash count analysis\n",
    "\n",
    "> The simple plot reveals some intersting data points.\n",
    ">> * As part of the plot, the % of change is added. The initial % of change it very large. Data collection appears to really be from 2015 forward. There is some data from 2012->2015 but it appears to be incomplete.\n",
    ">>    * **Using % can be misleading because looking at 2014 to 2015, the change is massive.** Does that mean that Chicago was having an immediate accident crisis? No. The data is more likely incomplete withing the dataset and an idicator that the data from 2016 forward is really a better starting point for analysis.\n",
    ">>    * The spike between 2016 and 2017 may have been influenced by a well known baseball team breaking a 108 year streak of not winning a world series and an increase of visitors to Wrigley Field from all over the country the following year.\n",
    ">>    * Additional factors could be a lower gas price in 2017 that increased drivers visiting the city\n",
    ">>    * https://data.theadvertiser.com/gas-price/chicago/YORD/2023-02-06/  \n",
    ">> * 2020 has an interesting dip\n",
    ">>   * What could cause this? The global covid-19 pandemic resulted in a global slowdown and near stoppage of human movement. We can see that impact to the city for events and in office work and how the influenced a lower nummber of car crashes.\n",
    ">>   * [Chicago traffic, CTA patterns dramatically affected by COVID pandemic](https://abc7chicago.com/chicago-traffic-construction-cta-bus-tracker/10451178/)\n",
    ">> * Data collection for 2025 is ongoing and running this workbook at later points in 2025 or beyond may contain additional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa526d8-5bb8-4dc8-ba94-ed39b8311c7d",
   "metadata": {},
   "source": [
    "### Figure 2 - Injury density by hour of the day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98d7f7-c1bf-48dd-84c4-576ed8b25de6",
   "metadata": {},
   "source": [
    "> With our crashe data, we can in general there are a lot of crashes. Bar plots make the difficult. Similar to having a single Bin size. So we want to look our data at a smaller scale. The hour by day plot with a jitter can help us in understand what hours have mor observations of incidents.\n",
    "> * Questions we can look to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5ea8b-5b9a-4543-91a2-741c8ffd912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the reusable function will evaluate all years and filter initially on crashes with >=1 injuries\n",
    "\n",
    "# all injuries\n",
    "plot_crash_hour_of_day_vs_injuries_with_jitter(df_chicago_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c5f85-c426-4edd-9c19-dee52559d5de",
   "metadata": {},
   "source": [
    "> We can see that irrespective of hour the day, across all years, < 5 injuries is heavily saturated in observed incidents. One of the challenges of a jitter visualizations stems from the zoom level on the data being evaluated. In the initial jitter plot, we are looking at all years and >= 1 injuries. If we scope it down two fold by year an the minimal injury count, we should be able zoom into the data and get a better idea if patterns emerge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bbdb06-7666-419a-a6f5-0c81b9621642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the reusable function to enable the pywidgets to filter on year and injury counts\n",
    "# this will setup an options section with panel and display in a row 2 interactive filters\n",
    "# filter 1: year (bound to the unique list of all years within the dataset\n",
    "# filter 2: injury count (static range of 1 to 30)\n",
    "# as the user interacts wtih the filters the selected year and injury count will filter the dataset and replot the data\n",
    "setup_interactive_jitter(df_chicago_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d4bd9-d8ba-4c71-8320-48c9c1c36e3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Crash hour analysis\n",
    "\n",
    "> In overall, the hourly crashes to total injuries shows consentrated rate when the injury count 1 or more. Now if we look at 2023 as an exmple and ask the question of \"In 2023, what hours had 4 or more injuries, we see that the afternoon hours (13 - 16) demonstrate a cluster of injuries. Rush hours of employees leaving the office of the day is most logical conclusion. \n",
    "\n",
    "\n",
    "> * What hours should I avoid?\n",
    "> * 3pm to 6pm have a visibly larger number of injuries. \n",
    "\n",
    "> **Image found in static/2023_minimal_4_injury.png**\n",
    "\n",
    "> ![alt text](static/2023_minimal_4_injury.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260b1ab-087f-4ff3-a484-cd0a23247ced",
   "metadata": {},
   "source": [
    "### Figure 3 - Probability Distribution (Hour of Day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79305410-f80f-45f5-939c-bafaec9f0051",
   "metadata": {},
   "source": [
    "As we examined the jtter, it raises a question about the hour of day and the overall probability of an car accident occurring. To help answer that question we can utilize a histograme in combination with a kernel density esitimation and a normal distribution. \n",
    "\n",
    "We can look to anwer questions like:\n",
    "* Filtering by year, are  there times of the day that pose more risk for a car accident?\n",
    "* Filtering by year, what level of kurtosis is present in the data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4329d9-7e91-404a-a059-c6d6274efb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the reusable function to setup a widget control histogram\n",
    "# filter 1: year (bound to the unique list of all years within the dataset\n",
    "# plot a histogram of the crash data for the selected year\n",
    "# calculate the normal distribution and kurtosis value\n",
    "setup_histogram_crashes_by_year(df_chicago_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa775c57-8c13-4290-a3a6-833d9f0603d6",
   "metadata": {},
   "source": [
    "#### Probabilty analysis\n",
    "\n",
    "> Where do we see the most risk traveling in the city in vehicle?\n",
    "> * We can see using a kernel densisity with normal distribution, the time morning rush hours of 7 to 8 am pose a range of risk in the morning. We can also see the afternoon hours starting from 3pm to 6 or 7pm are the next risk windows.\n",
    "> When can we have travel with the lowest risk?\n",
    "> * If we have to travel to the city of Chicago and we want to minimize our risk, we should trave to and from the city between 10 and leave before 3pm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58793b4-e0e6-4fee-8ec0-659405fc893f",
   "metadata": {},
   "source": [
    "### Figure 4 - Heatmap Weather Conditions to Road Conditions\n",
    "\n",
    "To help use understand a possible source of car crashes. We can utilize two data categories prp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066014ce-d090-4b60-a174-2ed7c623354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the reusable function to setup a widget powered plot to allow you filter by year\n",
    "# this method is specifically targeting the road conditions in comparison to the \n",
    "# filter 1: year (bound to the unique list of all years within the dataset\n",
    "# plot a heatmap for the selected year with specific catergorical data: Weather and Road surface conditions.\n",
    "setup_heatmap_weather_road_condition_by_year(df_chicago_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b1b75-49e5-4cba-9a21-b645d628913b",
   "metadata": {},
   "source": [
    "#### Heatmap Analysis\n",
    "\n",
    "> The heatmap reveals an interesting property of the data and possible assumptions. \n",
    "> * Does the weather or road condition play role?\n",
    "> * Looking at various years, the large number of accidents occur in dry and clear conditions. The dataset lacks additional data around the volume of traffic at peak windows of traffic accidents.\n",
    "> * 2013 and 2014 are clearly not valid data years. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2e154-b574-49ca-b9ce-354c9ecd9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's call our reusable function that will analyze the probability by hour of the \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65323455-16f9-4ac5-86e2-5858d96a1c02",
   "metadata": {
    "panel-layout": {
     "height": 50.7969,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## Rules Adherence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c06b92-4a66-4028-a034-3c51867467de",
   "metadata": {},
   "source": [
    "> As part of building a reusable and repeatable workbook, we want to adhere to rules that will help us a data scientists collect, present, and share our findings with peers. As part of this workbook, these are the key rules that helped guide the final product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b0278-f832-49da-8c0a-88e655cc987f",
   "metadata": {
    "panel-layout": {
     "height": 44.09375,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "### Rule 1 (Telling a story)\n",
    "\n",
    "> Knowing our audience and how to craft computational narrative is key. The rule on story telling is about knowing our audience and presenting complex layers of data in a meaningful way that convey information. As part of this workbook, as the data is presented, I want to provide information around what is the intent of the presented model, how it was produced and present possible questions they can answer from the data. Like a story, I aimed to explain what problems that I might have encountered and steps I needed to take. This will help establish the how, the what and why.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0bd1b-a9a8-4b49-bbce-af830627e733",
   "metadata": {},
   "source": [
    "### Rule 2 (Document the process)\n",
    "\n",
    "> The documentation rule is about providing the context and flow of where I got the data, how the data was transformerd, and loaded into the environment. Similar to a term paper or other historical stories, we want to provide the footnotes on where we sourced information and how we obtained it. This is important to establish legitimacy on the outputs. As part of this workbook, context around specific processing are called out to help educate the reader and provide them samples and sources as well that they can reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b68bc-caf6-4a7a-a39e-ad2ef9f3b7b0",
   "metadata": {},
   "source": [
    "### Rule 3 (Use Cell Divisions)\n",
    "\n",
    "> The rule of cell division helps us establish a cleaner set of areas in the workbook. Each cell can act as boundary of information and points of focus. To help keep cells clean, functional areas that can call a function help reduce the initial UI noise. We want to keep cells from spanning multiple scrolls to provide a user a section by section viewing experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656671cb-00ac-44ab-86c1-14d12a9e2a5f",
   "metadata": {},
   "source": [
    "### Rule 4 (Code Modularization)\n",
    "\n",
    "> The rule for code moderation is  intended to keep complex code out of the direct view as well as give us clean separation of concerns. Creating concise, reusable, and parameterized functions allow the visual side of the workbook to stay more in presenting the data. The module code within the workbook directory helps us keep that clean. As part of this workbook, I created modules that worked in extract, transform, and loading spaces. The modules are part of the repo and also contain documentation within them to provide other users the ability to see how certain techniques are applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2660b9-90a8-4b5b-bcc5-e9348975a871",
   "metadata": {
    "panel-layout": {
     "height": 129.640625,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "### Rule 6 (Version Control)\n",
    "\n",
    "> The ability to restore and save a working set of code to demonstrate functionality is important in the developement community. Version control systems like Git or SubVersion offer that ability to store backups and branches of code that can worked on by different team members. Systems like Git provide free accounts to users to upload and share their work with the global community.\n",
    "As part of this workbook, all the source code is available in a publicly available repository on Git and can be accessed at [siads_521_assignment_3](https://github.com/rmasud-michigan/siads_521_assignment_3.git)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4a4d7-eb27-44e7-8e48-84dd349fb220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "panel-cell-order": [
   "e8cd5a83-874d-4f15-8ead-e22db3dc06d4",
   "95c7a6dc-47ca-4521-80bc-aa3b02b8ff89",
   "579f480f-2cc2-42c3-ac14-f70f37d357e8",
   "75fcdfdb-ac1d-428f-808b-0a4d7c04ee40",
   "07a25a98-effc-448e-b6f2-aaf3f0baa85f",
   "8786769d-4f9a-422d-8b12-fc4cc1b5c67f",
   "65323455-16f9-4ac5-86e2-5858d96a1c02",
   "997b0278-f832-49da-8c0a-88e655cc987f",
   "fd2660b9-90a8-4b5b-bcc5-e9348975a871"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
